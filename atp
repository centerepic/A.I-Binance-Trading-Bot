# -*- coding: utf-8 -*-
"""
Created on Mon Jul 22 06:58:25 2024

@author: azzy
"""
"""
how to test
1.geta tokens data
2. plot the graphs of indicators and combine them
3.look at the graph
4.write simple nerual network that predicts the graph training on the pervious data
5.plot the predictions and original
6. look at the graph
7.check if i want to store the weights or relearn everytime

building phase
1.find indicator that can show movement in graphs of specfic time interval (ADX,Zscore)
2.get the top moving tokens and arrange in descending order
3.preform regression or whatever the method is to predicto 50/50
4.arrange the tokens based on the longest time spent on either side of the 50/50
5.trade with the tokens arranged in ascending order
6.start traded
7.(this is looping) continue to predict and learn from the latest incoming values
8.close the trade
9.rinse and repeat
"""
from keys import *
# importing keys
import pandas as pd
import numpy as np #computing multidimensionla arrays
from datetime import datetime
from time import sleep
from binance.client import Client
from binance import *
from binance.enums import *
import os
from sklearn.linear_model import LinearRegression
from sklearn.neural_network import MLPClassifier
import joblib
from scipy.integrate import trapezoid  # Correct function
globalInterval=Client.KLINE_INTERVAL_15MINUTE
########close/open trades###########
def Lsafe(client,Seed,mrgType,lvrg):
    try:
        client.futures_change_leverage(symbol=Seed,leverage=lvrg)
        client.futures_change_margin_type(symbol=Seed,marginType=mrgType)
    except:
        return
    
#Precession
def get_current_datetime_as_string():
    current_datetime = datetime.now()
    return current_datetime.strftime("%Y-%m-%d %H:%M:%S")

def truncate(number, precision):
    factor = 10.0 ** precision
    return int(number * factor) / factor

def LongOrder(client, Seed, precision, numBots, lvrg):
    balance = client.futures_account_balance()
    bal = None
    
    for wc in balance:
        if wc["asset"] == 'USDT':
            bal = float(wc["balance"])
            break

    if bal is None:
        return "No USDT balance found"

    percent = 0.9 / numBots  # Calculate the percentage of balance to use for each bot

    price = float(client.futures_mark_price(symbol=Seed)["markPrice"])
    maxl = (bal * percent) * lvrg
    maxq = maxl / price
    q = truncate(maxq, precision)

    try:
        result=client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_BUY, quantity=str(q))
        if result['orderId']:
            return str(q)
        else:
            return 'null'
    except Exception as e:
        print("ShortOrder",e)
        return 'null'
    
def ShortOrder(client, Seed, precision, numBots, lvrg):
    balance = client.futures_account_balance()
    bal = None
    
    for wc in balance:
        if wc["asset"] == 'USDT':
            bal = float(wc["balance"])
            break

    if bal is None:
        return "No USDT balance found"

    percent = 0.9 / numBots  # Calculate the percentage of balance to use for each bot

    price = float(client.futures_mark_price(symbol=Seed)["markPrice"])
    maxl = (bal * percent) * lvrg
    maxq = maxl / price
    q = truncate(maxq, precision)

    try:
        result=client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_SELL, quantity=str(q))
        if result['orderId']:
            return str(q)
        else:
            return 'null'
    except Exception as e:
        print("ShortOrder",e)
        return 'null'
        

def closeLong(client, p, Seed):
    try:
        client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_SELL, quantity=p, reduceOnly='true')
        return f"Closed long position with quantity {p}"
    except:
        return 'null'

def closeShort(client, p, Seed):
    try:
        client.futures_create_order(symbol=Seed, type=ORDER_TYPE_MARKET, side=SIDE_BUY, quantity=p, reduceOnly='true')
        return f"Closed short position with quantity {p}"
    except:
        return 'null'
    
def TradeLog():
    if not os.path.exists('TradeLog.csv'):
        # Create a DataFrame with the desired structure
        data = {
            'token': [],
            'datetime': [],
            'opprice': [],
            'result':[],
            'status': []
        }
        df = pd.DataFrame(data)
        df.to_csv('TradeLog.csv', index=False)
        return df
    else:
        df=pd.read_csv('TradeLog.csv')
        return df

def clear_tradeLog():
    try:
        os.remove("TradeLog.csv")
        print(" TradeLog.csv deleted successfully.")
    except FileNotFoundError:
        print("File TradeLog.csv not found.")
        pass
    except Exception as e:
        print(f"An error occurred: {e}")

def add_tradeLog(Ti,datetime, opprice,result,amount,status):
    dataframe=TradeLog()
    new_row = pd.DataFrame({
        'token': Ti,
        'datetime': datetime,
        'opprice': opprice,
        'amount': amount,
        'result': result,
        'status': status
    }, index=[0])  # Ensure it's a single row DataFrame
    
    dataframe = pd.concat([dataframe, new_row], ignore_index=True)
    dataframe.to_csv('TradeLog.csv', index=False)
    
def update_cell_Tlog(token, column_name, new_value):
    df=TradeLog()
    row_index = df.index[df['token'] == token ].tolist()
    if not row_index:
        print(f"Indicator '{token}' not found.")
        return
    column_index = df.columns.get_loc(column_name)
    df.iloc[row_index[0], column_index] = new_value
    df.to_csv('TradeLog.csv', index=False)
    return True

def read_matching_TLog(token, column_name):
    df=TradeLog()
    row_index = df.index[df['token'] == token].tolist()
    if not row_index:
        print(f"Indicator '{token}' not found.")
        return
    column_index = df.columns.get_loc(column_name)
    cell_value = df.iloc[row_index[0], column_index]
    return cell_value



# Updated calculate_peak_areas function
def calculate_peak_areas(data):
    """
    This function takes a pandas Series as input, identifies the peaks and troughs, and calculates the area under each peak.
    Returns a list of the area under each peak.
    """
    x_values = np.arange(len(data))
    y_values = data.values
    
    # Identify the peaks by finding where the slope changes from positive to negative
    peaks = (np.diff(np.sign(np.diff(y_values))) < 0).nonzero()[0] + 1
    
    # Calculate areas under each peak using scipy.integrate.trapezoid
    areas = []
    for i in range(len(peaks) - 1):
        area = trapezoid(y_values[peaks[i]:peaks[i+1]], x_values[peaks[i]:peaks[i+1]])
        areas.append(area)
    
    return areas

def calculate_sequential_cumulative_sum(data):
    """
    This function calculates the sequential cumulative sum of consecutive positive or negative areas.
    Each value in the resulting sequence represents consecutive positive or negative areas added together.
    """
    # Calculate the areas under each peak/trough
    peak_areas = calculate_peak_areas(data)
    
    # Calculate the sequential cumulative sum for consecutive positive or negative areas
    sequential_sum = []
    cumulative_sum = 0
    current_sign = np.sign(peak_areas[0])
    
    for area in peak_areas:
        if np.sign(area) == current_sign:
            cumulative_sum += area
        else:
            sequential_sum.append(cumulative_sum)
            cumulative_sum = area
            current_sign = np.sign(area)
    
    # Append the last cumulative sum
    sequential_sum.append(round(cumulative_sum, 2))
    
    return sequential_sum

def sequential_cumulative_sum(data):
    """
    This function calculates the sequential cumulative sum of consecutive positive or negative areas
    and returns the resulting list.
    """
    return calculate_sequential_cumulative_sum(data)


# Classifier functions

def train_and_process_large_magnitude(series, window_size=5, model_file='large_magnitude_model.pkl'):
    if len(series) < window_size + 1:
        raise ValueError("Not enough data to train the model")

    X, y = [], []
    for i in range(len(series) - window_size):
        window = series[i:i + window_size]
        future_value = series[i + window_size]
        X.append(window)
        y.append(1 if future_value > 0 else 0)  # 1 for positive trend, 0 for negative trend

    X = np.array(X)
    y = np.array(y)

    # Ensure X is a 2D array
    if X.ndim == 1:
        X = X.reshape(-1, 1)

    try:
        model = joblib.load(model_file)
    except FileNotFoundError:
        model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='tanh', max_iter=1000, random_state=42)

    model.fit(X, y)
    joblib.dump(model, model_file)

    return model

def detect_large_magnitude(series, window_size=5, model_file='large_magnitude_model.pkl'):
    model = joblib.load(model_file)
    
    if len(series) < window_size + 1:
        return 0.0, 0.0  # Return 0 probabilities if there is not enough data

    X = []
    for i in range(len(series) - window_size):
        window = series[i:i + window_size]
        X.append(window)
    
    X = np.array(X)
    
    # Ensure X is a 2D array
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    probabilities = model.predict_proba(X)
    
    positive_probability = probabilities[-1, 1]
    negative_probability = probabilities[-1, 0]
    
    return positive_probability, negative_probability

def train_and_process_small_magnitude(series, window_size=5, model_file='small_magnitude_model.pkl'):
    if len(series) < window_size + 1:
        raise ValueError("Not enough data to train the model")

    threshold = np.std(series)  # Threshold for small magnitude
    X, y = [], []
    for i in range(len(series) - window_size):
        window = series[i:i + window_size]
        future_value = series[i + window_size]
        is_small_magnitude = abs(future_value) < threshold  # Small magnitude detection
        X.append(window)
        y.append(1 if is_small_magnitude else 0)

    X = np.array(X)
    y = np.array(y)

    # Ensure X is a 2D array
    if X.ndim == 1:
        X = X.reshape(-1, 1)

    try:
        model = joblib.load(model_file)
    except FileNotFoundError:
        model = MLPClassifier(hidden_layer_sizes=(100, 50), activation='tanh', max_iter=1000, random_state=42)
        
    model.fit(X, y)
    joblib.dump(model, model_file)

    return model

def detect_small_magnitude(series, window_size=5, model_file='small_magnitude_model.pkl'):
    model = joblib.load(model_file)
    
    if len(series) < window_size + 1:
        return 0.0  # Return 0 probability if there is not enough data

    X = []
    for i in range(len(series) - window_size):
        window = series[i:i + window_size]
        X.append(window)
    
    X = np.array(X)
    
    # Ensure X is a 2D array
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    probabilities = model.predict_proba(X)
    
    small_magnitude_probability = probabilities[-1, 1]  # Probability of small magnitude
    
    return small_magnitude_probability

# Combining large and small magnitude probabilities

def get_directional_probability(series):
    positive_prob, negative_prob = detect_large_magnitude(series)
    small_magnitude_prob = detect_small_magnitude(series)
    
    # Refine probabilities by subtracting the small magnitude probability
    positive_probability = positive_prob - small_magnitude_prob
    negative_probability = negative_prob - small_magnitude_prob
    
    # Cap probabilities to a maximum of 0.9 and minimum of 0
    positive_probability = max(min(positive_probability, 0.9), 0)
    negative_probability = max(min(negative_probability, 0.9), 0)
    
    return positive_probability, negative_probability
#################### Get token info #########################

def batchCollector(excludedlist,CurrencyType):
    client = Client(api_key, api_secret)
    exInfo=client.futures_exchange_info()
    tokenInf,sinf=FindNewToken(client,exInfo,excludedlist,CurrencyType)
    Ftoken_list=OpenLayer(tokenInf)
    return Ftoken_list,sinf

def TokenInfo(client,sym,CurrencyType):
    try:
        candles = client.futures_continous_klines(pair=sym, interval=globalInterval,ContractType='PERPETUAL')
        df = pd.DataFrame(candles)
        df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
        df['timestart'] = df['timestart'] / 1000
        df['timeend'] = df['timeend'] / 1000
        df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
        return df
    except Exception as e:
        #TokenInfo error Removed from Market or not added yet
        print("TokenInfo error Removed from Market or not added yet",e)

def FindNewToken(client,exInfo,excludedlist,CurrencyType):
    symInfo={}
    tokenInfo={}
    for symbol in exInfo["symbols"]:
        if symbol["contractType"]=="PERPETUAL" and symbol["symbol"] not in excludedlist and CurrencyType in symbol["symbol"] and symbol["status"]=="TRADING":
            symInfo[symbol["symbol"]]=symbol["quantityPrecision"]
    x=0
    for key,values in symInfo.items():
        result=TokenInfo(client,key,CurrencyType)
        tokenInfo[key]=result
        if x==30:
            print("30%")
        if x==80:
            print("55%")
        if x==130:
            print("69%")
        x+=1           
    return tokenInfo,symInfo

def OpenLayer(TokenInfo):
    
    ranked_result={}
    for Ti,df in TokenInfo.items():
        try:        
            cmf_series=CMF(df)
            rsi_series=rsi(df)
            macd_series=MACD(df)
            # Combine the normalized and smoothed series
            series = (cmf_series + rsi_series + macd_series) / 3
            series=sequential_cumulative_sum(series)
            #series = count_consecutive_values(series)
            # Train the network
            #neuralStuff.train_network(series)
            # Train the model
            # Preprocess the data
            #train_ongoing_cluster_model(series)
            # Train the model (this will update the model if it already exists)
            train_and_process_large_magnitude(series)
            train_and_process_small_magnitude(series)
            # Train the model and save it
            #train_ongoing_cluster_model(X, y)

            #model_cont, model_next = train_gbdt(series, look_back=1, epochs=1000)
        except Exception as e:
            #TokenInfo error Removed from Market or not added yet
            print("Trainloop error",e)
            continue
    print("90%")
    for Ti,df in TokenInfo.items():
        try:        
            cmf_series=CMF(df)
            rsi_series=rsi(df)
            macd_series=MACD(df)
            # Combine the normalized and smoothed series
            series = (cmf_series + rsi_series + macd_series) / 3
            series=sequential_cumulative_sum(series)
            # train_and_process_large_magnitude(series)
            # train_and_process_small_magnitude(series)
            positive_prob, negative_prob = get_directional_probability(series)
            pp=round(positive_prob,3)
            pn=round(negative_prob,3)
            #print(Ti,pp,pn)
            if pp>pn:
                ranked_result[Ti]=[pp,'FL']
            if pn>pp:
                ranked_result[Ti]=[pn,'FS']
        except Exception as e:
            #TokenInfo error Removed from Market or not added yet
            print("predictloop error",e)
            continue   
    # Sort the keys based on the first element of the list in descending order
    sorted_keys = sorted(ranked_result.keys(), key=lambda x: ranked_result[x][0],reverse=True)
    # Reorganize the dictionary using the sorted keys
    sorted_ranked_result = {key: ranked_result[key] for key in sorted_keys}
    print("100%")
    return sorted_ranked_result

#################### Get token info #########################

############# Indicators #############

def CMF(data):
    period = 20
    mf = ((data['close'] - data['low']) - (data['high'] - data['close'])) / (data['high'] - data['low'])
    mfv = mf * data['volume']
    cmf_values = mfv.rolling(period).sum() / data['volume'].rolling(period).sum()
    
    # Initialize the full index
    full_index = np.arange(len(data))
    
    # Fill NaN values using linear regression
    original_series = cmf_values
    filled_series = original_series.reindex(full_index)

    # Indices of non-NaN values
    not_nan_indices = np.where(~np.isnan(filled_series))[0]
    # Indices of NaN values
    nan_indices = np.where(np.isnan(filled_series))[0]

    # Perform linear regression if there are NaN values
    if len(nan_indices) > 0:
        reg = LinearRegression()
        if len(not_nan_indices) > 1:  # Only fit if there are enough points
            reg.fit(not_nan_indices.reshape(-1, 1), filled_series.dropna().values)
            # Predict NaN values
            filled_series[nan_indices] = reg.predict(nan_indices.reshape(-1, 1))
        
        # If filled_series is still NaN at index 0, fill it with the first non-NaN value
        if np.isnan(filled_series[0]):
            filled_series[0] = filled_series.dropna().iloc[0]

    normalized_series = 2 * (filled_series - filled_series.min()) / (filled_series.max() - filled_series.min()) - 1
    return normalized_series


def rsi(dataset, period=14):
    # Calculate daily returns
    delta = dataset['close'].diff()
    
    # Separate positive and negative gains/losses
    gain = delta.where(delta > 0, 0)
    loss = -delta.where(delta < 0, 0)
    
    # Calculate average gain and loss
    avg_gain = gain.rolling(window=period, min_periods=1).mean()
    avg_loss = loss.rolling(window=period, min_periods=1).mean()
    
    # Calculate the Relative Strength (RS)
    rs = avg_gain / avg_loss
    
    # Calculate the RSI
    rsi = 100 - (100 / (1 + rs))
    normalized_rsi = 2 * (rsi - 50) / 100
    
    # Fill NaN values using linear regression
    original_series = normalized_rsi
    full_index = np.arange(len(dataset))
    filled_series = original_series.reindex(full_index)

    # Indices of non-NaN values
    not_nan_indices = np.where(~np.isnan(filled_series))[0]
    # Indices of NaN values
    nan_indices = np.where(np.isnan(filled_series))[0]

    # Perform linear regression if there are NaN values
    if len(nan_indices) > 0:
        reg = LinearRegression()
        if len(not_nan_indices) > 1:  # Only fit if there are enough points
            reg.fit(not_nan_indices.reshape(-1, 1), filled_series.dropna().values)
            # Predict NaN values
            filled_series[nan_indices] = reg.predict(nan_indices.reshape(-1, 1))
        
        # If filled_series is still NaN at index 0, fill it with the first non-NaN value
        if np.isnan(filled_series[0]):
            filled_series[0] = filled_series.dropna().iloc[0]

    normalized_series = 2 * (filled_series - filled_series.min()) / (filled_series.max() - filled_series.min()) - 1
    return normalized_series

def calculate_ema(series, period):
    return series.ewm(span=period, adjust=False).mean()

def MACD(dataset, short_period=12, long_period=26, signal_period=9):
    # Calculate the short-term EMA (12-day EMA)
    short_ema = calculate_ema(dataset['close'], short_period)
    
    # Calculate the long-term EMA (26-day EMA)
    long_ema = calculate_ema(dataset['close'], long_period)
    
    # Calculate the MACD line
    macd_line = short_ema - long_ema
    
    # Calculate the Signal line (9-day EMA of the MACD line)
    signal_line = calculate_ema(macd_line, signal_period)
    
    # Calculate the MACD Histogram
    macd_histogram = macd_line - signal_line
    
    # Normalize the MACD Histogram to a range from -100 to 100
    max_abs_histogram = macd_histogram.abs().max()
    normalized_histogram = (macd_histogram / max_abs_histogram) * 100
    
    # Fill NaN values using linear regression
    original_series = normalized_histogram
    full_index = np.arange(len(dataset))
    filled_series = original_series.reindex(full_index)

    # Indices of non-NaN values
    not_nan_indices = np.where(~np.isnan(filled_series))[0]
    # Indices of NaN values
    nan_indices = np.where(np.isnan(filled_series))[0]

    # Perform linear regression if there are NaN values
    if len(nan_indices) > 0:
        reg = LinearRegression()
        if len(not_nan_indices) > 1:  # Only fit if there are enough points
            reg.fit(not_nan_indices.reshape(-1, 1), filled_series.dropna().values)
            # Predict NaN values
            filled_series[nan_indices] = reg.predict(nan_indices.reshape(-1, 1))
        
        # If filled_series is still NaN at index 0, fill it with the first non-NaN value
        if np.isnan(filled_series[0]):
            filled_series[0] = filled_series.dropna().iloc[0]

    normalized_series = 2 * (filled_series - filled_series.min()) / (filled_series.max() - filled_series.min()) - 1
    return normalized_series

def z_score(series):
    return (series - series.mean()) / series.std()

def moving_average(series, window=14):
    return series.rolling(window=window, min_periods=1).mean()

def all_w_or_l(column_name, df):
    # Check if the column exists in the DataFrame
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' does not exist in the DataFrame.")
    
    # Check if all values in the column are either 'W' or 'L'
    if (df[column_name].isin(['W', 'L'])).all():
        return True
    else:
        return False
    
############# Indicators #############    
########close/open trades########### add_tradeLog(Ti,datetime, opprice,result,amount,status,EXP)
def Rtrade(bList,BotLimit,lvrg,tinfo,mode):
    client = Client(api_key, api_secret)
    mrgType="ISOLATED"
    x=0
    for Ti,ttype in bList.items():
        Lsafe(client, Ti, mrgType, lvrg)
        candles = client.futures_continous_klines(pair=Ti, interval=globalInterval,ContractType='PERPETUAL')
        df = pd.DataFrame(candles)
        df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
        df['timestart'] = df['timestart'] / 1000
        df['timeend'] = df['timeend'] / 1000
        df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
        if ttype[1]=="FL":
            opprice = df['close'].iloc[-1]
            datetime=get_current_datetime_as_string()
            if mode=='R':
                p=LongOrder(client,Ti,tinfo[Ti],BotLimit,lvrg)
                if p!="null" and p!='':    
                    print("added:",Ti,ttype)
                    add_tradeLog(Ti,datetime,opprice,'A',p,ttype[1])
                    x+=1
            else:
                print("added:",Ti,ttype)
                add_tradeLog(Ti,datetime,opprice,'A','nan',ttype[1])
                x+=1
        if  ttype[1]=="FS":
            opprice = df['close'].iloc[-1]
            datetime=get_current_datetime_as_string()
            if mode=='R':
                p=ShortOrder(client,Ti,tinfo[Ti],BotLimit,lvrg)
                if p!="null" and p!='':
                    print("added:",Ti,ttype)
                    add_tradeLog(Ti,datetime,opprice,'A',p,ttype[1])
                    x+=1
            else:
                print("added:",Ti,ttype)
                add_tradeLog(Ti,datetime,opprice,'A','nan',ttype[1])
                x+=1
        if x>=BotLimit:
            break
        sleep(5)
    while True:
        TLog=TradeLog()
        for x in range(len(TLog)):
            status=TLog.iloc[x]['status']
            svresult=TLog.iloc[x]['result']
            Ti=TLog.iloc[x]['token']
            amount=read_matching_TLog(Ti, 'amount')
            oprice=read_matching_TLog(Ti, 'opprice')
            if svresult!='W' and svresult!='L':
                candles = client.futures_continous_klines(pair=Ti, interval=globalInterval,ContractType='PERPETUAL')
                df = pd.DataFrame(candles)
                df.columns = ['timestart', 'open', 'high', 'low', 'close', 'volume', 'timeend', 'quote_asset_volume', 'number_of_trades', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'unused_field']
                df['timestart'] = df['timestart'] / 1000
                df['timeend'] = df['timeend'] / 1000
                df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']] = df[['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']].astype(float)
                cmf_series=CMF(df)
                rsi_series=rsi(df)
                macd_series=MACD(df)
                cprice = df['close'].iloc[-1]
                # Combine the normalized and smoothed series
                series = (cmf_series + rsi_series + macd_series) / 3
                series=sequential_cumulative_sum(series)
                # train_and_process_large_magnitude(series)
                # train_and_process_small_magnitude(series)
                positive_prob, negative_prob = get_directional_probability(series)
                pp=round(positive_prob,3)
                pn=round(negative_prob,3)
                if status=='FS' and pp>pn:
                    if oprice>cprice:
                        if mode=='R':
                            closeShort(client,amount,Ti)
                        update_cell_Tlog(Ti, 'result', 'W')
                        print("trade closed : ", Ti,oprice,cprice, 'W')
                        break
                    else:
                        if mode=='R':
                            closeShort(client,amount,Ti)
                        update_cell_Tlog(Ti, 'result', 'L')
                        print("trade closed : ", Ti,oprice,cprice, 'L')
                        break        
                if status=='FL' and pn>pp:
                    if oprice<cprice:
                        if mode=='R':
                            closeLong(client,amount,Ti)
                        update_cell_Tlog(Ti, 'result', 'W')
                        print("trade closed : ", Ti,oprice,cprice, 'W')
                        break
                    else:
                        if mode=='R':
                            closeLong(client,amount,Ti)
                        update_cell_Tlog(Ti, 'result', 'L')
                        print("trade closed : ",Ti,oprice,cprice, 'L')
                        break        
        TLog=TradeLog()
        if all_w_or_l('result', TLog):
            print("All Trades Done")
            return True
        sleep(900)
    print("All Trades Done")
    return True
########trainer############
def MAIN_TRADER(excludedlist,CurrencyType,BotLimit,lvrg,initial,mode):
    client = Client(api_key, api_secret)
    while True:
        balance = client.futures_account_balance()
        bal = None
        for wc in balance:
            if wc["asset"] == 'USDT':
                bal = float(wc["balance"])
        if bal > initial:
            amnt = bal - initial
            client.futures_account_transfer(asset='USDT', amount=amnt, Type=2)
            print("Profit Transfer:", amnt)
        if bal > 100:
            BotLimit=4
        if bal > 500:
            BotLimit=8
        if bal > 900:
            BotLimit=10
        TokenList,tinfo = batchCollector(excludedlist,CurrencyType)
        print('tokens found:',len(TokenList))
        if len(TokenList)>0:
            Rtrade(TokenList,BotLimit,lvrg,tinfo,mode)
        clear_tradeLog()
        print("sleeping waiting for next cycle")
clear_tradeLog()
BotLimit=2
mode='R'
print("Booting up... ")
excludedlist=['BTCUSDT','BTCDOMUSDT','USDCUSDT','ETHUSDT','XEMUSDT']
CurrencyType="USDT"
lvrg=2
initial_amt=1000
MAIN_TRADER(excludedlist,CurrencyType,BotLimit,lvrg,initial_amt,mode)
